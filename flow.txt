================================================================================
                    Data Validator Agent MULTI-AGENT SYSTEM
                        PROJECT FLOW DOCUMENTATION
================================================================================

TABLE OF CONTENTS:
1. System Overview
2. High-Level Architecture
3. Component Breakdown
4. Complete Data Flow (Start to End)
5. Agent Details and Interactions
6. Execution Engine Deep Dive
7. API Endpoints
8. Database Schema
9. Security Architecture
10. Configuration and Deployment

================================================================================
1. SYSTEM OVERVIEW
================================================================================

PROJECT NAME: CSV Analysis Multi-Agent System (VAGENT)
PURPOSE: Automated CSV data analysis using multiple AI agents orchestrated
         through LangGraph to validate, plan, execute, and synthesize insights

TECHNOLOGY STACK:
- Backend: FastAPI (Python)
- Frontend: Streamlit
- AI Framework: LangGraph + LangChain
- LLM: Claude Sonnet 4.5 (Anthropic)
- Security: RestrictedPython sandbox
- Database: SQLite
- Data Processing: Pandas, NumPy, Scikit-learn, Numba

KEY FEATURES:
✓ Multi-agent PEV (Plan-Execute-Verify) architecture
✓ Sandboxed code execution with RestrictedPython
✓ Fast-path optimization for simple queries
✓ Confidence scoring and sanity checks
✓ HITL (Human-in-the-Loop) detection for critical decisions
✓ Comprehensive audit logging
✓ Rate limiting and quota management
✓ PII redaction

================================================================================
2. HIGH-LEVEL ARCHITECTURE
================================================================================

                        ┌─────────────────────────┐
                        │     USER INTERFACE      │
                        │   (Streamlit Frontend)  │
                        │   app/frontend/app.py   │
                        └────────────┬────────────┘
                                         │
                                         │ HTTP REST API
                                         ▼
                         ┌─────────────────────────┐
                         │     API GATEWAY                │
                         │   (FastAPI Backend)            │
                         │  app/backend/main.py           │
                         └────────────┬────────────┘
                                          │
                    ┌────────────────┼────────────────┐
                    │                     │                     │
                    ▼                     ▼                    ▼
         ┌──────────────────┐  ┌─────────┐  ┌──────────────┐
         │  Submit Endpoint      │  │ Result     │  │   Database       │
         │  POST /submit         │  │ Endpoint   │  │   (SQLite)       │
         └────────┬─────────┘  └─────────┘  └──────────────┘
                    │
                    │ Triggers
                    ▼
         ┌─────────────────────────────────────────────┐
         │         ORCHESTRATOR (LangGraph)            │
         │         app/backend/agents/orchestrator.py  │
         │                                             │
         │    ┌──────────────────────────────────┐    │
         │    │   PEV State Machine Workflow     │    │
         │    └──────────────────────────────────┘    │
         └────────────────────┬────────────────────────┘
                                    │
                                    │ Coordinates
                                    ▼
         ┌──────────────────────────────────────────────┐
         │           AGENT ECOSYSTEM                                  │
         │                                                            │
         │  ┌──────────┐  ┌──────────┐  ┌───────────┐      │
         │  │Validation   │  │ Planning    │  │ Execution    │      │
         │  │  Agent      │  │  Agent      │  │   Agent      │      │
         │  └──────────┘  └──────────┘  └───────────┘      │
         │                                                            │
         │  ┌──────────┐  ┌──────────┐                  		  │
         │  │ Response    │  │  Plan       │          				  │
         │  │  Agent      │  │Validation   │          				  │
         │  └──────────┘  └──────────┘                		  │
         └──────────────────────────────────────────────┘
                              │
                              │ Uses
                              ▼
         ┌──────────────────────────────────────────────┐
         │          SUPPORTING MODULES                  │
         │                                              │
         │  ┌──────────┐  ┌──────────┐  ┌───────────┐ │
         │  │ Security │  │   Tools  │  │  Database │ │
         │  │  Module  │  │ (Sandbox)│  │   CRUD    │ │
         │  └──────────┘  └──────────┘  └───────────┘ │
         └──────────────────────────────────────────────┘

================================================================================
3. COMPONENT BREAKDOWN
================================================================================

3.1 FRONTEND LAYER
-------------------
File: app/frontend/app.py

Components:
1. Form Interface
   - Request Type dropdown (Price Strategy, Growth & Marketing, etc.)
   - Question text area (max 2000 chars)
   - Context text area (optional)
   - File uploader (CSV only, ≤10MB)
   - Priority selector (normal/urgent)
   - Submit button

2. Polling Mechanism
   - After submission, polls /api/form/result/{id} every 10s
   - Timeout: 120 seconds
   - Loading indicator displayed

3. Result Display
   - Success: Summary, confidence, insights, risks, next steps
   - Error: Validation failures with suggestions
   - Technical details in collapsible JSON view


3.2 BACKEND LAYER
------------------
File: app/backend/main.py

FastAPI Application:
- CORS middleware (allows Streamlit origin)
- API router mounting (/api/form)
- Database session management
- Startup event handlers

Routers:
File: app/backend/routers/submit.py

Endpoints:
1. POST /api/form/submit
   - Accept multipart/form-data or JSON
   - Validate file size (≤10MB), row count (<50k)
   - Save files to uploads/ directory
   - Create submission record in DB
   - Trigger orchestrator
   - Return submission ID and status

2. GET /api/form/result/{sub_id}
   - Poll for analysis results
   - Return current status and result JSON


3.3 DATABASE LAYER
-------------------
Files: app/database/models.py, app/database/crud.py

Schema: submissions table
- id: UUID (primary key)
- request_type: String
- question: Text
- context: Text (nullable)
- priority: String (normal/urgent)
- deep_analysis: Boolean (deprecated)
- status: String (pending/done/error)
- files: JSON array of file paths
- result: Text (JSON result bundle)
- created_at: DateTime
- updated_at: DateTime

CRUD Operations:
- create_submission(): Insert new record
- get_submission(id): Fetch by ID
- update_result(id, status, result): Update status and result


3.4 ORCHESTRATOR
-----------------
File: app/backend/agents/orchestrator.py

Purpose: LangGraph state machine orchestrating PEV workflow

State: OrchestrationState
- submission_id: UUID
- payload: Input data (question, files, priority, etc.)
- validation: Validation agent output
- plan: Planning agent output
- plan_validation: Plan validation output
- execution: Execution agent output
- response: Response agent output
- hitl_required: Boolean
- error: Error message (if any)
- status: Workflow status

Nodes:
1. validate_node: Call validation agent
2. plan_node: Call planning agent
3. validate_plan_node: Check plan-data compatibility
4. execute_node: Call execution agent
5. verify_node: Call response agent

Edges:
- validate → plan (if ok) or END (if error)
- plan → validate_plan
- validate_plan → execute (if ok) or END (if missing columns)
- execute → verify
- verify → END

Key Function:
process_submission(sub_id):
  1. Load submission from DB
  2. Run PEV workflow graph
  3. Aggregate results into result bundle
  4. Update DB with status and result
  5. Return result


3.5 AGENT ECOSYSTEM
--------------------

AGENT 1: VALIDATION AGENT
File: app/backend/agents/validation.py

Purpose: Data quality gatekeeper

Functions:
- profile_data(file_path):
  Returns: {shape, columns, dtypes, missing_counts, numeric_stats,
           sample_rows, empty_cols, all_null_cols}

- profile_and_validate(files):
  Checks: file existence, CSV format, row count limits
  Returns: profiles dict or error

- validate_submission(request_type, question, context, profiles):
  Uses LLM to check:
    ✓ Is data relevant to the question?
    ✓ Is data quality sufficient?
    ✓ Are there critical issues?
  Returns: {ok, message, suggestions}

Output Structure:
{
  "ok": true/false,
  "message": "Validation passed" or error description,
  "suggestions": ["Fix missing values in column X", ...],
  "profiles": {
    "file1.csv": {...profile data...},
    ...
  }
}


AGENT 2: PLANNING AGENT
File: app/backend/agents/planning.py

Purpose: Strategic task planner

Functions:
- plan_analysis(request_type, question, context, profiles):
  Uses LLM to generate structured plan

- get_mock_plan(): Returns mock plan for testing

Output Structure:
{
  "task_type": "simulate|predict|analyze|optimize",
  "steps": [
    {
      "id": "load_data",
      "action": "load_data",
      "params": {"file": "data.csv"},
      "description": "Load CSV data"
    },
    {
      "id": "group_aggregate",
      "action": "group_by_aggregate",
      "params": {
        "group_by": ["region"],
        "aggregations": {"revenue": ["sum", "mean"]}
      },
      "description": "Calculate revenue by region"
    }
  ],
  "required_columns": ["region", "revenue"],
  "expected_outputs": ["average_revenue_by_region"],
  "failure_modes": ["missing_columns", "insufficient_data"]
}


AGENT 3: PLAN VALIDATION AGENT
File: app/backend/agents/orchestrator.py (validate_plan_node)

Purpose: Plan-data schema validator

Logic:
1. Extract required_columns from plan
2. Get available columns from data profiles
3. Check if all required columns exist
4. Return ok=true or ok=false with missing columns list

Output Structure:
{
  "ok": true/false,
  "message": "All required columns present" or "Missing columns: X, Y",
  "missing_columns": ["col1", "col2"] or []
}


AGENT 4: EXECUTION AGENT
File: app/backend/agents/execution.py

Purpose: Code execution engine with ReAct reasoning

Architecture: ReAct Loop (Reason-Act-Observe)

Components:
1. ExecutionState:
   - plan: Execution plan
   - current_step: Step index
   - outputs: Results from previous steps
   - shared_env: Persistent execution environment (df, variables)
   - error: Error message
   - retry_count: Retry counter
   - completed: Boolean

2. Tools:
   - code_execution: RestrictedPython sandbox (pandas, numpy, sklearn)
   - monte_carlo_numba: High-performance simulations (Numba JIT)
   - rag_query: Document retrieval (FAISS - stubbed)

3. Fast-Path Optimization:
   For simple queries (e.g., "average revenue by region"):
   - Detect patterns: "region" + "revenue" + "average"
   - Direct pandas computation without LLM
   - Saves cost and latency (0.1s vs 30-60s)

4. ReAct Workflow:

   ┌────────────────────────────────────────────┐
   │           ENTRY: reason_node               │
   └────────────────┬───────────────────────────┘
                    │
                    ▼
   ┌────────────────────────────────────────────┐
   │        REASON NODE (LLM Decision)          │
   │  - Analyze current step from plan          │
   │  - Decide which tool to use                │
   │  - Generate Python code                    │
   │  - Set params for execution                │
   └────────────────┬───────────────────────────┘
                    │
                    ▼
   ┌────────────────────────────────────────────┐
   │         ACT NODE (Execute)                 │
   │  - Run tool (code_execution, etc.)         │
   │  - Update shared_env with results          │
   │  - Store outputs                           │
   │  - Handle exceptions                       │
   └────────────────┬───────────────────────────┘
                    │
                    ▼
   ┌────────────────────────────────────────────┐
   │       OBSERVE NODE (Check Results)         │
   │  - Check for errors                        │
   │  - Increment retry count if error          │
   │  - Move to next step if success            │
   │  - Decide: continue, retry, or end         │
   └────────────────┬───────────────────────────┘
                    │
                    ▼
              ┌─────────────┐
              │  Decision   │
              └─────┬───┬───┘
                    │   │
          Error &   │   │   Success or
       retries < 3  │   │   max retries
                    │   │
                    ▼   ▼
              ┌─────┐ ┌─────┐
              │RETRY│ │ END │
              │ to  │ └─────┘
              │REASON
              └─────┘

5. State Persistence:
   - shared_env dictionary maintains state across steps
   - Example:
     Step 1: df = pd.read_csv('data.csv')
     Step 2: avg = df.groupby('region')['revenue'].mean()
            (df persists from step 1)

6. Security: RestrictedPython Sandbox
   - Blocks: open(), __import__(), eval(), exec(), file I/O
   - Allows: pandas, numpy, sklearn, numba, math
   - Timeout: 160 seconds per execution
   - Import statements stripped (modules pre-injected)

Output Structure:
{
  "outputs": {
    "step_id_1": {"result": "...", "data": {...}},
    "step_id_2": {"result": "...", "data": {...}}
  },
  "error": null or "error message",
  "completed": true/false
}


AGENT 5: RESPONSE AGENT
File: app/backend/agents/response.py

Purpose: Insight synthesizer and verifier

Function: generate_response(request_type, question, priority,
                            validation, plan, execution)

Logic:
1. Aggregate all results from previous agents
2. Use LLM to synthesize structured response
3. Calculate confidence score based on:
   - Data quality (validation results)
   - Execution success
   - Statistical consistency
4. Perform sanity checks:
   - Check bounds and units
   - Validate statistical consistency
   - Flag anomalies
5. Detect HITL requirements:
   - High-impact decisions (e.g., "Strategic Decisions")
   - Low confidence (<0.7)
   - Critical errors

Output Structure:
{
  "tldr": "2-3 sentence summary" (only if priority=urgent),
  "summary": "Comprehensive 3-5 sentence analysis",
  "confidence": 0.85 (float 0.0-1.0),
  "insights": [
    "Key finding 1",
    "Key finding 2",
    ...
  ],
  "sanity_checks": {
    "bounds_valid": true,
    "units_consistent": true,
    "statistical_consistency": "passed"
  },
  "risks": [
    "Assumption: Linear trend holds",
    "Limitation: Only 6 months of data"
  ],
  "disclaimers": [
    "Based on 10,000 rows",
    "Outliers removed"
  ],
  "next_steps": [
    "Collect more recent data",
    "Validate assumptions with domain experts"
  ],
  "artifacts": {} (removed in current version)
}


3.6 SUPPORTING MODULES
-----------------------

SECURITY MODULE
File: app/backend/agents/security.py

Functions:
1. redact_pii(text):
   - Pattern-based redaction for:
     ✓ Email addresses
     ✓ Phone numbers
     ✓ SSN
     ✓ Credit card numbers
   - Returns text with [REDACTED] placeholders

2. log_audit_event(event_type, submission_id, metadata):
   - Logs to audit_logs/audit_YYYY-MM-DD.jsonl
   - Format: One JSON per line
   - Fields: timestamp, event_type, submission_id,
            prompt_hash, response_hash, metadata

3. check_quota(user_id):
   - In-memory quota tracker
   - Limits: 100 submissions/hour (configurable)
   - Returns: available quota count or raises QuotaExceeded


TOOLS MODULE
File: app/backend/tools/cost_estimation.py

Functions:
1. estimate_cost(prompt_tokens, completion_tokens):
   - Claude Sonnet 4.5 pricing
   - Returns: estimated cost in USD

2. estimate_execution_time(plan):
   - Estimates based on:
     ✓ Number of steps
     ✓ Complexity of operations
     ✓ Fast-path eligible
   - Returns: estimated seconds


DATABASE MODULE
File: app/database/database.py

Functions:
1. get_db():
   - SQLAlchemy session factory
   - Yields database session
   - Handles connection lifecycle

2. init_db():
   - Creates all tables
   - Sets up database schema


================================================================================
4. COMPLETE DATA FLOW (START TO END)
================================================================================

STEP 1: USER INTERACTION
-------------------------
Location: Streamlit Frontend (app/frontend/app.py)

User Actions:
1. Opens browser to http://localhost:8501
2. Fills form:
   - Selects Request Type: "Price Strategy"
   - Enters Question: "What is the average revenue by region?"
   - (Optional) Adds Context
   - Uploads CSV file: sales_data.csv (5MB, 10,000 rows)
   - Selects Priority: "normal"
3. Clicks "Submit" button

Frontend Processing:
- Validates file size (≤10MB) ✓
- Prepares multipart/form-data payload
- Sends POST request to http://127.0.0.1:8000/api/form/submit


STEP 2: API GATEWAY
--------------------
Location: FastAPI Backend (app/backend/routers/submit.py)

Validation:
1. Check file size: 5MB ≤ 10MB ✓
2. Read CSV with pandas: Valid format ✓
3. Check row count: 10,000 < 50,000 ✓
4. Check quota: 50/100 submissions used ✓

Processing:
1. Generate UUID: sub_id = "abc-123-def-456"
2. Save file: uploads/abc-123-def-456_sales_data.csv
3. Create DB record:
   - id: abc-123-def-456
   - request_type: Price Strategy
   - question: "What is the average revenue by region?"
   - files: ["uploads/abc-123-def-456_sales_data.csv"]
   - status: "pending"
   - created_at: 2025-12-14 10:30:00

4. Log audit event:
   audit_logs/audit_2025-12-14.jsonl:
   {"timestamp": "2025-12-14T10:30:00Z", "event_type": "submission_created",
    "submission_id": "abc-123-def-456", "metadata": {...}}

5. Trigger orchestrator: process_submission("abc-123-def-456")

Response to Frontend:
{
  "id": "abc-123-def-456",
  "status": "pending",
  "result": null
}


STEP 3: ORCHESTRATOR INITIALIZATION
------------------------------------
Location: app/backend/agents/orchestrator.py

Function: process_submission("abc-123-def-456")

1. Load submission from DB:
   submission = get_submission("abc-123-def-456")

2. Initialize state:
   state = {
     "submission_id": "abc-123-def-456",
     "payload": {
       "request_type": "Price Strategy",
       "question": "What is the average revenue by region?",
       "context": "",
       "priority": "normal",
       "files": ["uploads/abc-123-def-456_sales_data.csv"]
     },
     "validation": None,
     "plan": None,
     "plan_validation": None,
     "execution": None,
     "response": None,
     "hitl_required": False,
     "error": None,
     "status": "pending"
   }

3. Create LangGraph workflow:
   - Define nodes (validate, plan, validate_plan, execute, verify)
   - Define edges (conditional routing)
   - Compile graph

4. Execute workflow: final_state = graph.invoke(state)


STEP 4: VALIDATION AGENT
--------------------------
Location: app/backend/agents/validation.py

Node: validate_node(state)

Processing:
1. Profile CSV:
   profile_data("uploads/abc-123-def-456_sales_data.csv")

   Result:
   {
     "shape": [10000, 4],
     "columns": ["region", "product", "revenue", "date"],
     "dtypes": {
       "region": "object",
       "product": "object",
       "revenue": "float64",
       "date": "object"
     },
     "missing_counts": {
       "region": 0,
       "product": 0,
       "revenue": 50,
       "date": 0
     },
     "numeric_stats": {
       "revenue": {
         "mean": 5000.0,
         "std": 2000.0,
         "min": 100.0,
         "max": 15000.0
       }
     },
     "sample_rows": [...first 5 rows...],
     "empty_cols": [],
     "all_null_cols": []
   }

2. LLM Validation:
   validate_submission(
     request_type="Price Strategy",
     question="What is the average revenue by region?",
     context="",
     profiles={"sales_data.csv": {...profile...}}
   )

   LLM Prompt:
   """
   You are a data validation expert. Check if this data is relevant
   for the user's question.

   Request Type: Price Strategy
   Question: What is the average revenue by region?

   Data Profile:
   - Columns: region, product, revenue, date
   - Rows: 10,000
   - Missing: 50 values in revenue (0.5%)

   Validate:
   1. Is data relevant?
   2. Is data quality sufficient?
   3. Are there critical issues?

   Return JSON: {ok, message, suggestions}
   """

   LLM Response:
   {
     "ok": true,
     "message": "Data is highly relevant. Contains region and revenue
                 columns needed for analysis. Missing values are minimal
                 (0.5%) and can be handled.",
     "suggestions": [
       "Consider imputing or removing 50 missing revenue values",
       "Verify date format for temporal analysis"
     ]
   }

3. Update state:
   state["validation"] = {
     "ok": true,
     "message": "...",
     "suggestions": [...],
     "profiles": {...}
   }

Decision: ok=true → Route to plan_node


STEP 5: PLANNING AGENT
------------------------
Location: app/backend/agents/planning.py

Node: plan_node(state)

Processing:
1. Extract context:
   - Request Type: Price Strategy
   - Question: What is the average revenue by region?
   - Data Profile: 10,000 rows, columns [region, revenue]

2. LLM Planning:
   plan_analysis(
     request_type="Price Strategy",
     question="What is the average revenue by region?",
     context="",
     profiles={...}
   )

   LLM Prompt:
   """
   You are a data analysis planner. Generate a structured plan
   to answer the user's question.

   Question: What is the average revenue by region?

   Available Data:
   - Columns: region, product, revenue, date
   - Rows: 10,000

   Generate a plan with:
   - task_type: analyze/predict/simulate/optimize
   - steps: [{id, action, params, description}]
   - required_columns: [...]
   - expected_outputs: [...]
   - failure_modes: [...]

   Return JSON.
   """

   LLM Response:
   {
     "task_type": "analyze",
     "steps": [
       {
         "id": "load_data",
         "action": "load_data",
         "params": {
           "file": "uploads/abc-123-def-456_sales_data.csv"
         },
         "description": "Load sales data from CSV"
       },
       {
         "id": "clean_data",
         "action": "clean_data",
         "params": {
           "columns": ["revenue"],
           "method": "dropna"
         },
         "description": "Remove rows with missing revenue values"
       },
       {
         "id": "group_aggregate",
         "action": "group_by_aggregate",
         "params": {
           "group_by": ["region"],
           "aggregations": {
             "revenue": ["mean", "sum", "count"]
           }
         },
         "description": "Calculate average revenue by region"
       }
     ],
     "required_columns": ["region", "revenue"],
     "expected_outputs": ["average_revenue_by_region"],
     "failure_modes": ["missing_columns", "non_numeric_revenue"]
   }

3. Update state:
   state["plan"] = {...plan...}

Decision: Route to validate_plan_node


STEP 6: PLAN VALIDATION AGENT
-------------------------------
Location: app/backend/agents/orchestrator.py

Node: validate_plan_node(state)

Processing:
1. Extract required columns from plan:
   required_columns = ["region", "revenue"]

2. Extract available columns from validation profiles:
   available_columns = ["region", "product", "revenue", "date"]

3. Check intersection:
   missing = set(required_columns) - set(available_columns)
   # missing = {} (empty set)

4. Update state:
   state["plan_validation"] = {
     "ok": true,
     "message": "All required columns present",
     "missing_columns": []
   }

Decision: ok=true → Route to execute_node


STEP 7: EXECUTION AGENT
-------------------------
Location: app/backend/agents/execution.py

Node: execute_node(state)

Fast-Path Check:
1. Analyze question: "What is the average revenue by region?"
2. Detect keywords: "region" ✓, "revenue" ✓, "average" ✓
3. Fast-path eligible: YES

Fast-Path Execution:
```python
import pandas as pd

# Load data
df = pd.read_csv("uploads/abc-123-def-456_sales_data.csv")

# Clean data
df = df.dropna(subset=['revenue'])

# Calculate average revenue by region
result = df.groupby('region')['revenue'].mean().to_dict()

# Output
outputs = {
  "average_revenue_by_region": result
}
```

Result:
```json
{
  "average_revenue_by_region": {
    "North": 5200.50,
    "South": 4800.30,
    "East": 5500.75,
    "West": 4900.20
  }
}
```

Execution Time: 0.1 seconds (vs 30-60s for ReAct loop)

Alternative: ReAct Loop (if not fast-path eligible)
---------------------------------------------------

If question was complex (e.g., "Predict next quarter revenue by region"):

ITERATION 1:

1. REASON NODE:
   LLM analyzes step "load_data":
   Decision:
   {
     "tool": "code_execution",
     "code": "import pandas as pd\ndf = pd.read_csv('uploads/...')",
     "description": "Load CSV data"
   }

2. ACT NODE:
   Execute code in RestrictedPython sandbox:
   - Inject modules: pandas, numpy
   - Run code
   - Capture result: df (DataFrame with 10,000 rows)
   - Store in shared_env: {"df": df}

   Update state:
   state["outputs"]["load_data"] = {
     "result": "Loaded 10,000 rows",
     "shape": [10000, 4]
   }

3. OBSERVE NODE:
   Check: error=None ✓
   Action: Move to next step (current_step = 1)

ITERATION 2:

1. REASON NODE:
   LLM analyzes step "clean_data":
   Decision:
   {
     "tool": "code_execution",
     "code": "df = df.dropna(subset=['revenue'])",
     "description": "Remove missing revenue values"
   }

2. ACT NODE:
   Execute code:
   - Access shared_env: df (from previous step)
   - Run code
   - Update shared_env: {"df": cleaned_df}

   Update state:
   state["outputs"]["clean_data"] = {
     "result": "Removed 50 rows with missing revenue",
     "shape": [9950, 4]
   }

3. OBSERVE NODE:
   Check: error=None ✓
   Action: Move to next step (current_step = 2)

ITERATION 3:

1. REASON NODE:
   LLM analyzes step "group_aggregate":
   Decision:
   {
     "tool": "code_execution",
     "code": "result = df.groupby('region')['revenue'].mean().to_dict()",
     "description": "Calculate average revenue by region"
   }

2. ACT NODE:
   Execute code:
   - Access shared_env: df
   - Run code
   - Capture result

   Update state:
   state["outputs"]["group_aggregate"] = {
     "result": {...averages by region...}
   }

3. OBSERVE NODE:
   Check: error=None ✓, all steps completed ✓
   Action: Set completed=True, END

Update State:
state["execution"] = {
  "outputs": {...all outputs...},
  "error": null,
  "completed": true
}

Decision: Route to verify_node


STEP 8: RESPONSE AGENT
------------------------
Location: app/backend/agents/response.py

Node: verify_node(state)

Processing:
1. Aggregate results:
   - Validation: ok=true, data quality good
   - Plan: 3-step analysis plan
   - Execution: successful, averages computed
   - Outputs: {"average_revenue_by_region": {...}}

2. LLM Synthesis:
   generate_response(
     request_type="Price Strategy",
     question="What is the average revenue by region?",
     priority="normal",
     validation={...},
     plan={...},
     execution={...}
   )

   LLM Prompt:
   """
   You are a data analysis expert. Synthesize results into a
   structured response.

   Question: What is the average revenue by region?

   Execution Results:
   - North: $5,200.50 average revenue
   - South: $4,800.30 average revenue
   - East: $5,500.75 average revenue
   - West: $4,900.20 average revenue

   Generate:
   - summary: 3-5 sentence analysis
   - confidence: 0.0-1.0 score
   - insights: key findings
   - sanity_checks: validation of results
   - risks: assumptions and limitations
   - disclaimers: data caveats
   - next_steps: recommendations

   Return JSON.
   """

   LLM Response:
   {
     "tldr": null (priority=normal, so no TLDR),
     "summary": "The analysis reveals significant regional revenue
                 variation across four regions. The East region leads
                 with an average revenue of $5,500.75, followed by North
                 ($5,200.50), West ($4,900.20), and South ($4,800.30).
                 This 15% gap between highest and lowest suggests
                 region-specific pricing or market dynamics.",
     "confidence": 0.88,
     "insights": [
       "East region generates 15% higher revenue than South region",
       "North and East regions outperform South and West",
       "Revenue distribution suggests potential for targeted pricing
        strategies in underperforming regions"
     ],
     "sanity_checks": {
       "bounds_valid": true,
       "units_consistent": true,
       "statistical_consistency": "passed"
     },
     "risks": [
       "Assumption: Historical patterns will continue",
       "Limitation: Analysis based on 6-month data window",
       "50 rows (0.5%) with missing revenue were excluded"
     ],
     "disclaimers": [
       "Based on 9,950 rows after removing missing values",
       "Regional boundaries assumed to be stable",
       "Currency units not verified"
     ],
     "next_steps": [
       "Investigate drivers of East region's superior performance",
       "Consider dynamic pricing for South and West regions",
       "Collect more recent data to validate trends"
     ],
     "artifacts": {}
   }

3. Confidence Calculation:
   - Data quality: 0.95 (only 0.5% missing)
   - Execution success: 1.0 (no errors)
   - Statistical consistency: 0.9 (reasonable variance)
   - Weighted average: 0.88

4. HITL Detection:
   - Request type: Price Strategy (not Strategic Decisions)
   - Confidence: 0.88 (≥ 0.7 threshold)
   - Decision: hitl_required = false

5. Update state:
   state["response"] = {...response...}
   state["hitl_required"] = false

Decision: Route to END


STEP 9: ORCHESTRATOR FINALIZATION
-----------------------------------
Location: app/backend/agents/orchestrator.py

Function: process_submission (continued)

1. Workflow completed, extract final state

2. Build result bundle:
   result = {
     "validation": {...},
     "plan": {...},
     "execution": {...},
     "response": {...},
     "hitl_required": false,
     "status": "done"
   }

3. Update database:
   update_result(
     submission_id="abc-123-def-456",
     status="done",
     result=json.dumps(result)
   )

4. Log audit event:
   audit_logs/audit_2025-12-14.jsonl:
   {"timestamp": "2025-12-14T10:30:45Z", "event_type": "submission_completed",
    "submission_id": "abc-123-def-456", "status": "done",
    "confidence": 0.88}

5. Return result to API endpoint


STEP 10: FRONTEND POLLING
---------------------------
Location: Streamlit Frontend (app/frontend/app.py)

Polling Loop:
1. Start polling immediately after submission
2. Send GET request: /api/form/result/abc-123-def-456
3. Check status:
   - "pending": Continue polling (wait 10s, retry)
   - "done": Display results
   - "error": Display error message

4. Timeout: 120 seconds (12 polls)

API Response (after completion):
{
  "id": "abc-123-def-456",
  "status": "done",
  "result": {
    "validation": {...},
    "plan": {...},
    "execution": {...},
    "response": {
      "summary": "The analysis reveals...",
      "confidence": 0.88,
      "insights": [...],
      "risks": [...],
      "disclaimers": [...],
      "next_steps": [...]
    }
  }
}


STEP 11: RESULT DISPLAY
-------------------------
Location: Streamlit Frontend (app/frontend/app.py)

Display Components:

1. Success Banner:
   ✓ Analysis completed successfully (Confidence: 88%)

2. Summary Section:
   The analysis reveals significant regional revenue variation across
   four regions. The East region leads with an average revenue of
   $5,500.75, followed by North ($5,200.50), West ($4,900.20), and
   South ($4,800.30). This 15% gap between highest and lowest suggests
   region-specific pricing or market dynamics.

3. Key Insights:
   • East region generates 15% higher revenue than South region
   • North and East regions outperform South and West
   • Revenue distribution suggests potential for targeted pricing
     strategies in underperforming regions

4. Risks & Limitations:
   ⚠ Assumption: Historical patterns will continue
   ⚠ Limitation: Analysis based on 6-month data window
   ⚠ 50 rows (0.5%) with missing revenue were excluded

5. Disclaimers:
   ℹ Based on 9,950 rows after removing missing values
   ℹ Regional boundaries assumed to be stable
   ℹ Currency units not verified

6. Next Steps:
   → Investigate drivers of East region's superior performance
   → Consider dynamic pricing for South and West regions
   → Collect more recent data to validate trends

7. Technical Details (Collapsible):
   - Execution Plan: [JSON view]
   - Full Result: [JSON view]
   - Validation Report: [JSON view]

END OF FLOW
-----------
Total Time: ~45 seconds (with LLM calls) or ~5 seconds (fast-path)


================================================================================
5. AGENT DETAILS AND INTERACTIONS
================================================================================

AGENT INTERACTION DIAGRAM
--------------------------

                        ┌─────────────────────┐
                        │    ORCHESTRATOR     │
                        │   (LangGraph FSM)   │
                        └──────────┬──────────┘
                                   │
                ┌──────────────────┼──────────────────┐
                │                  │                  │
                ▼                  ▼                  ▼
      ┌─────────────────┐  ┌─────────────┐  ┌────────────────┐
      │   VALIDATION         │  │  PLANNING       │  │   EXECUTION  		│
      │     AGENT            │  │    AGENT        │  │     AGENT      		│
      │                      │  │                 │  │   (ReAct)      		│
      │  Profiles CSV        │  │ Generates       │  │  Sandboxed     		│
      │  Validates           │  │ Multi-step      │  │  Python Code   		│
      │  Relevance           │  │ Plan with       │  │  Execution     		│
      │  (LLM)               │  │ LLM             │  │  with State    		│
      │                      │  │                 │  │  Persistence   		│
      │  Early Reject        │  │ Task Type       │  │                		│
      │  on Errors           │  │ Steps           │  │  Fast-path    		│
      │                      │  │ Columns         │  │  Optimization 	    │
      └────────┬────────┘  └──────┬──────┘  └────────┬───────┘
               │                         │                       │
               │ Passes profiles         │                       │
               └──────────────────►                        │
                                  │                              │
                                  │ Passes plan                  │
                                  └──────────────────►
                                                     │
                                                     │ Passes outputs
                                                     ▼
                                          ┌──────────────────┐
                                          │   PLAN VALIDATION│
                                          │   (Schema Check) │
                                          │                  │
                                          │  Verifies columns│
                                          │  exist in data   │
                                          └────────┬─────────┘
                                                   │
                                                   │ Passes validation
                                                   ▼
                                          ┌──────────────────┐
                                          │   RESPONSE       │
                                          │     AGENT        │
                                          │  (Synthesizer)   │
                                          │                  │
                                          │  Aggregates all  │
                                          │  results         │
                                          │                  │
                                          │  LLM Synthesis   │
                                          │  Confidence      │
                                          │  Sanity Checks   │
                                          │  HITL Detection  │
                                          └──────────────────┘


AGENT STATE TRANSITIONS
-------------------------

State: OrchestrationState

Initial State:
{
  submission_id: "abc-123",
  payload: {request_type, question, files, ...},
  validation: null,
  plan: null,
  plan_validation: null,
  execution: null,
  response: null,
  hitl_required: false,
  error: null,
  status: "pending"
}

After VALIDATION AGENT:
{
  ...same...,
  validation: {ok: true/false, message, suggestions, profiles},
  status: "validating"
}

After PLANNING AGENT:
{
  ...same...,
  plan: {task_type, steps, required_columns, expected_outputs},
  status: "planning"
}

After PLAN VALIDATION:
{
  ...same...,
  plan_validation: {ok: true/false, message, missing_columns},
  status: "validating_plan"
}

After EXECUTION AGENT:
{
  ...same...,
  execution: {outputs, error, completed},
  status: "executing"
}

After RESPONSE AGENT:
{
  ...same...,
  response: {summary, confidence, insights, risks, next_steps, ...},
  hitl_required: true/false,
  status: "done"
}


CONDITIONAL ROUTING LOGIC
---------------------------

1. After validate_node:
   if state["validation"]["ok"] == true:
     → plan_node
   else:
     → END (early rejection)

2. After plan_node:
   → validate_plan_node (always)

3. After validate_plan_node:
   if state["plan_validation"]["ok"] == true:
     → execute_node
   else:
     → END (missing columns)

4. After execute_node:
   → verify_node (always, even if errors for error reporting)

5. After verify_node:
   → END (always)


ERROR HANDLING
---------------

Each agent has try-except wrapper:

```python
def validate_node(state):
    try:
        validation = validate_submission(...)
        state["validation"] = validation
    except Exception as e:
        state["validation"] = {
            "ok": false,
            "message": f"Validation error: {str(e)}",
            "suggestions": []
        }
        state["error"] = str(e)
    return state
```

Errors are captured but workflow continues to response agent
for proper error reporting.


================================================================================
6. EXECUTION ENGINE DEEP DIVE
================================================================================

REACT LOOP ARCHITECTURE
------------------------

File: app/backend/agents/execution.py

ExecutionState Schema:
{
  "plan": {...execution plan...},
  "current_step": 0,
  "outputs": {},
  "shared_env": {},
  "error": null,
  "retry_count": 0,
  "completed": false
}


NODE 1: REASON NODE
-------------------

Purpose: LLM decides what to do for current step

Input: ExecutionState

Logic:
1. Get current step from plan:
   step = plan["steps"][current_step]
   # Example: {id: "load_data", action: "load_data", params: {...}}

2. Build LLM prompt:
   """
   You are executing step {step["id"]}: {step["description"]}

   Action: {step["action"]}
   Parameters: {step["params"]}

   Available tools:
   - code_execution: Run Python code (pandas, numpy, sklearn)
   - monte_carlo_numba: High-performance simulations
   - rag_query: Document retrieval

   Previous outputs: {state["outputs"]}
   Shared environment: {state["shared_env"].keys()}

   Decide:
   1. Which tool to use?
   2. What code to execute?
   3. What parameters to pass?

   Return JSON: {tool, code, description}
   """

3. LLM generates decision:
   {
     "tool": "code_execution",
     "code": "import pandas as pd\ndf = pd.read_csv('file.csv')",
     "description": "Load CSV data"
   }

4. Update state with decision

Output: ExecutionState with tool decision


NODE 2: ACT NODE
-----------------

Purpose: Execute LLM's decision

Input: ExecutionState with tool decision

Logic:
1. Extract decision:
   tool = state["decision"]["tool"]
   code = state["decision"]["code"]

2. Execute tool:

   if tool == "code_execution":
     result = execute_code_sandbox(code, state["shared_env"])

     Sandbox execution:
     a. Strip import statements (modules pre-injected)
     b. Create safe globals with RestrictedPython:
        safe_globals = {
          "__builtins__": safe_builtins,
          "pd": pandas,
          "np": numpy,
          "plt": matplotlib.pyplot,
          ...shared_env variables (df, etc.)...
        }
     c. Compile code with RestrictedPython
     d. Execute with timeout (160s)
     e. Capture result and updated variables
     f. Update shared_env with new variables

     Example:
       Input code: "df = pd.read_csv('file.csv')"
       After execution: shared_env["df"] = <DataFrame>

   elif tool == "monte_carlo_numba":
     result = run_monte_carlo_simulation(params, num_samples)

     Uses Numba JIT compilation for performance
     Example: 10,000 simulations in 2 seconds

   elif tool == "rag_query":
     result = query_faiss_index(query, top_k)
     (Currently stubbed)

3. Store result:
   state["outputs"][step_id] = {
     "result": result,
     "data": {...extracted data...}
   }

4. Handle errors:
   try:
     ...execution...
   except Exception as e:
     state["error"] = str(e)

Output: ExecutionState with execution results or error


NODE 3: OBSERVE NODE
---------------------

Purpose: Check results and decide next action

Input: ExecutionState with execution results

Logic:
1. Check for errors:
   if state["error"] is not None:
     state["retry_count"] += 1

     if state["retry_count"] < MAX_RETRIES (3):
       # Retry: go back to REASON with error context
       return "retry"
     else:
       # Max retries: give up, move to next step
       state["current_step"] += 1
       state["retry_count"] = 0
       return "continue" if more steps else "end"

   else:
     # Success: move to next step
     state["retry_count"] = 0
     state["current_step"] += 1

     if state["current_step"] >= len(state["plan"]["steps"]):
       state["completed"] = true
       return "end"
     else:
       return "continue"

Output: Routing decision (retry/continue/end)


CONDITIONAL EDGES
------------------

observe_node outputs:
- "retry": → reason_node (retry current step)
- "continue": → reason_node (next step)
- "end": → END (workflow complete)


FAST-PATH OPTIMIZATION
-----------------------

Function: run_execution(plan, files, question)

Fast-path detection:
```python
def should_use_fastpath(question, plan):
    # Pattern 1: Region-revenue average query
    if ("region" in question.lower() and
        "revenue" in question.lower() and
        ("average" in question.lower() or "avg" in question.lower())):
        return True

    # Pattern 2: Simple aggregation (future)
    # ...

    return False

if should_use_fastpath(question, plan):
    # Direct computation
    df = pd.read_csv(files[0])
    df = df.dropna(subset=['revenue'])
    result = df.groupby('region')['revenue'].mean().to_dict()

    return {
        "outputs": {"fast_result": result},
        "error": None,
        "completed": True
    }
else:
    # Use ReAct loop
    return run_react_loop(plan, files)
```

Benefits:
- Latency: 0.1s vs 30-60s
- Cost: $0 vs $0.50-2.00 per query
- Reliability: No LLM hallucination risk


SANDBOX SECURITY
-----------------

RestrictedPython Configuration:

Blocked Operations:
- File I/O: open(), read(), write()
- Network: urllib, requests, socket
- System: os.system(), subprocess
- Dangerous builtins: eval(), exec(), compile(), __import__()
- Module imports: Must be pre-injected

Allowed Operations:
- Data manipulation: pandas, numpy, sklearn
- Math: math, statistics
- Visualization: matplotlib (limited)
- High-performance: numba

Timeout:
- 160 seconds per execution
- Prevents infinite loops

Example Blocked Code:
```python
# BLOCKED: File I/O
with open('/etc/passwd', 'r') as f:
    data = f.read()

# BLOCKED: Network access
import urllib.request
response = urllib.request.urlopen('http://evil.com')

# BLOCKED: Dynamic import
module = __import__('os')
module.system('rm -rf /')

# BLOCKED: Eval
eval('malicious_code()')
```

Example Allowed Code:
```python
# ALLOWED: Data analysis
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

df = pd.read_csv('data.csv')
model = LinearRegression()
model.fit(df[['x']], df['y'])
predictions = model.predict(df[['x']])
```


STATE PERSISTENCE EXAMPLE
---------------------------

Step 1: Load Data
```python
# Code executed
df = pd.read_csv('sales.csv')

# shared_env after execution
{
  "df": <pandas.DataFrame with 10,000 rows>
}

# outputs
{
  "load_data": {
    "result": "Loaded 10,000 rows",
    "shape": [10000, 4]
  }
}
```

Step 2: Clean Data
```python
# Code executed (df available from shared_env)
df = df.dropna(subset=['revenue'])
df = df[df['revenue'] > 0]

# shared_env after execution
{
  "df": <pandas.DataFrame with 9,950 rows>
}

# outputs
{
  "load_data": {...},
  "clean_data": {
    "result": "Removed 50 rows",
    "shape": [9950, 4]
  }
}
```

Step 3: Analyze
```python
# Code executed (df available from shared_env)
avg_by_region = df.groupby('region')['revenue'].mean()
result = avg_by_region.to_dict()

# shared_env after execution
{
  "df": <pandas.DataFrame with 9,950 rows>,
  "avg_by_region": <pandas.Series>,
  "result": <dict>
}

# outputs
{
  "load_data": {...},
  "clean_data": {...},
  "analyze": {
    "result": {
      "North": 5200.50,
      "South": 4800.30,
      "East": 5500.75,
      "West": 4900.20
    }
  }
}
```


================================================================================
7. API ENDPOINTS
================================================================================

ENDPOINT 1: POST /api/form/submit
----------------------------------

Purpose: Submit CSV analysis request

Method: POST
Content-Type: multipart/form-data or application/json

Request Body (multipart):
- request_type: String (required)
  Values: "Price Strategy", "Growth & Marketing", "Customer Insights",
          "Market Intelligence", "Strategic Decisions"
- question: String (required, max 2000 chars)
- context: String (optional, max 5000 chars)
- priority: String (optional, default "normal")
  Values: "normal", "urgent"
- file: File (required, CSV format)
- deep_analysis: Boolean (optional, deprecated)

Request Body (JSON):
{
  "request_type": "Price Strategy",
  "question": "What is the average revenue by region?",
  "context": "We are planning Q1 2026 pricing",
  "priority": "normal",
  "files": ["path/to/file.csv"]
}

Validations:
1. File size: ≤ 10 MB
2. Row count: < 50,000 rows
3. File format: Valid CSV (readable by pandas)
4. Question length: ≤ 2000 chars
5. Quota: Check submission quota (100/hour)

Success Response (200 OK):
{
  "id": "abc-123-def-456",
  "status": "pending",
  "result": null
}

Error Responses:

400 Bad Request (Validation Error):
{
  "detail": "File size exceeds 10MB limit"
}

400 Bad Request (Invalid CSV):
{
  "detail": "File is not a valid CSV: ParserError"
}

400 Bad Request (Too Many Rows):
{
  "detail": "File has 60,000 rows, limit is 50,000"
}

429 Too Many Requests (Quota Exceeded):
{
  "detail": "Quota exceeded: 100 submissions per hour"
}

500 Internal Server Error:
{
  "detail": "Internal server error"
}


ENDPOINT 2: GET /api/form/result/{submission_id}
-------------------------------------------------

Purpose: Poll for analysis result

Method: GET
Path Parameters:
- submission_id: String (UUID)

Success Response (200 OK):

Case 1: Pending
{
  "id": "abc-123-def-456",
  "status": "pending",
  "result": null
}

Case 2: Completed
{
  "id": "abc-123-def-456",
  "status": "done",
  "result": {
    "validation": {
      "ok": true,
      "message": "Data is highly relevant",
      "suggestions": [...],
      "profiles": {...}
    },
    "plan": {
      "task_type": "analyze",
      "steps": [...],
      "required_columns": [...],
      "expected_outputs": [...],
      "failure_modes": [...]
    },
    "execution": {
      "outputs": {...},
      "error": null,
      "completed": true
    },
    "response": {
      "tldr": null,
      "summary": "The analysis reveals...",
      "confidence": 0.88,
      "insights": [...],
      "sanity_checks": {...},
      "risks": [...],
      "disclaimers": [...],
      "next_steps": [...]
    },
    "hitl_required": false,
    "status": "done"
  }
}

Case 3: Error
{
  "id": "abc-123-def-456",
  "status": "error",
  "result": {
    "validation": {
      "ok": false,
      "message": "Data is not relevant for question",
      "suggestions": ["Use sales data instead of HR data"]
    },
    "error": "Validation failed"
  }
}

Error Responses:

404 Not Found:
{
  "detail": "Submission not found"
}


TYPICAL USAGE FLOW
-------------------

Client-side pseudocode:

```javascript
// Step 1: Submit request
const formData = new FormData()
formData.append('request_type', 'Price Strategy')
formData.append('question', 'What is the average revenue by region?')
formData.append('priority', 'normal')
formData.append('file', csvFile)

const submitResponse = await fetch('http://localhost:8000/api/form/submit', {
  method: 'POST',
  body: formData
})

const { id, status } = await submitResponse.json()
// id = "abc-123-def-456", status = "pending"

// Step 2: Poll for result
let result = null
const maxPolls = 12  // 120 seconds
const pollInterval = 10000  // 10 seconds

for (let i = 0; i < maxPolls; i++) {
  await sleep(pollInterval)

  const resultResponse = await fetch(
    `http://localhost:8000/api/form/result/${id}`
  )
  const data = await resultResponse.json()

  if (data.status === 'done') {
    result = data.result
    break
  } else if (data.status === 'error') {
    throw new Error(data.result.error)
  }
  // else: status = "pending", continue polling
}

if (result === null) {
  throw new Error('Timeout: Analysis took longer than 120 seconds')
}

// Step 3: Display result
console.log(result.response.summary)
console.log(result.response.insights)
```


================================================================================
8. DATABASE SCHEMA
================================================================================

DATABASE: SQLite (db.sqlite)

TABLE: submissions
------------------

CREATE TABLE:
```sql
CREATE TABLE submissions (
    id VARCHAR PRIMARY KEY,           -- UUID
    request_type VARCHAR NOT NULL,    -- Request category
    question TEXT NOT NULL,           -- User question
    context TEXT,                     -- Additional context (nullable)
    priority VARCHAR DEFAULT 'normal',-- Priority level
    deep_analysis BOOLEAN DEFAULT 0,  -- Enable 10k sims (deprecated)
    status VARCHAR DEFAULT 'pending', -- Workflow status
    files JSON NOT NULL,              -- Array of file paths
    result TEXT,                      -- JSON result bundle (nullable)
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_status ON submissions(status);
CREATE INDEX idx_created_at ON submissions(created_at);
```

COLUMN DETAILS:

id (VARCHAR):
- UUID v4 format
- Generated by Python uuid.uuid4()
- Primary key
- Example: "abc-123-def-456"

request_type (VARCHAR):
- Allowed values:
  * "Price Strategy"
  * "Growth & Marketing"
  * "Customer Insights"
  * "Market Intelligence"
  * "Strategic Decisions"
- Not nullable

question (TEXT):
- User's question
- Max length: 2000 chars (enforced in API)
- Not nullable
- Example: "What is the average revenue by region?"

context (TEXT):
- Optional additional context
- Max length: 5000 chars (enforced in API)
- Nullable
- Example: "We are planning Q1 2026 pricing strategy"

priority (VARCHAR):
- Allowed values: "normal", "urgent"
- Default: "normal"
- Affects response format (TLDR for urgent)

deep_analysis (BOOLEAN):
- Deprecated feature
- Previously: Enable 10,000 Monte Carlo simulations
- Default: false (0)

status (VARCHAR):
- Workflow status
- Allowed values: "pending", "done", "error"
- Default: "pending"
- Updated by orchestrator

files (JSON):
- Array of file paths
- Format: ["uploads/abc-123_file1.csv", "uploads/abc-123_file2.csv"]
- Not nullable
- Example: ["uploads/abc-123-def-456_sales_data.csv"]

result (TEXT):
- JSON result bundle from orchestrator
- Nullable (null while pending)
- Large text field (can be >100KB)
- Structure:
  {
    "validation": {...},
    "plan": {...},
    "execution": {...},
    "response": {...},
    "hitl_required": false,
    "status": "done"
  }

created_at (TIMESTAMP):
- Submission creation time
- Auto-generated
- Used for quota tracking

updated_at (TIMESTAMP):
- Last update time
- Auto-updated on result update


CRUD OPERATIONS
---------------

File: app/database/crud.py

1. create_submission():
   Purpose: Insert new submission record
   Input: SubmissionCreate model
   Output: Submission model
   SQL: INSERT INTO submissions VALUES (...)

2. get_submission(db, sub_id):
   Purpose: Fetch submission by ID
   Input: Database session, submission ID
   Output: Submission model or None
   SQL: SELECT * FROM submissions WHERE id = ?

3. update_result(db, sub_id, status, result):
   Purpose: Update submission status and result
   Input: Database session, submission ID, status, result JSON
   Output: None
   SQL: UPDATE submissions SET status = ?, result = ?,
        updated_at = CURRENT_TIMESTAMP WHERE id = ?


EXAMPLE DATA
-------------

Submission Record (Pending):
{
  "id": "abc-123-def-456",
  "request_type": "Price Strategy",
  "question": "What is the average revenue by region?",
  "context": null,
  "priority": "normal",
  "deep_analysis": false,
  "status": "pending",
  "files": ["uploads/abc-123-def-456_sales_data.csv"],
  "result": null,
  "created_at": "2025-12-14T10:30:00.000Z",
  "updated_at": "2025-12-14T10:30:00.000Z"
}

Submission Record (Completed):
{
  "id": "abc-123-def-456",
  "request_type": "Price Strategy",
  "question": "What is the average revenue by region?",
  "context": null,
  "priority": "normal",
  "deep_analysis": false,
  "status": "done",
  "files": ["uploads/abc-123-def-456_sales_data.csv"],
  "result": "{\"validation\": {...}, \"response\": {...}, ...}",
  "created_at": "2025-12-14T10:30:00.000Z",
  "updated_at": "2025-12-14T10:30:45.000Z"
}


================================================================================
9. SECURITY ARCHITECTURE
================================================================================

SECURITY LAYERS
----------------

Layer 1: Input Validation
Layer 2: File System Isolation
Layer 3: Code Execution Sandbox
Layer 4: PII Redaction
Layer 5: Audit Logging
Layer 6: Rate Limiting


LAYER 1: INPUT VALIDATION
---------------------------

File: app/backend/routers/submit.py

Checks:
1. File Size: ≤ 10 MB
   if file.size > 10 * 1024 * 1024:
     raise HTTPException(400, "File size exceeds 10MB")

2. Row Count: < 50,000 rows
   if len(df) >= 50000:
     raise HTTPException(400, "File has too many rows")

3. File Format: Valid CSV
   try:
     pd.read_csv(file)
   except Exception as e:
     raise HTTPException(400, f"Invalid CSV: {e}")

4. Question Length: ≤ 2000 chars
   if len(question) > 2000:
     raise HTTPException(400, "Question too long")

5. Request Type: Allowed values
   if request_type not in ALLOWED_TYPES:
     raise HTTPException(400, "Invalid request type")


LAYER 2: FILE SYSTEM ISOLATION
--------------------------------

Upload Directory: uploads/
- All uploaded files stored in uploads/ directory
- File naming: {submission_id}_{original_filename}
- No path traversal: os.path.basename() used
- Example: uploads/abc-123_sales_data.csv

File Access:
- Only files in uploads/ accessible
- No absolute paths allowed in user input
- RestrictedPython blocks file I/O in executed code


LAYER 3: CODE EXECUTION SANDBOX
---------------------------------

File: app/backend/agents/execution.py

RestrictedPython Configuration:

1. Blocked Operations:
   - File I/O: open(), read(), write(), os.remove()
   - Network: urllib, requests, socket, http.client
   - System: os.system(), subprocess, eval(), exec()
   - Imports: __import__(), importlib
   - Dangerous builtins: compile(), delattr(), setattr()

2. Allowed Modules (Pre-injected):
   - pandas
   - numpy
   - sklearn (scikit-learn)
   - matplotlib (limited)
   - math
   - statistics
   - numba

3. Safe Builtins:
   - print (commented out due to RestrictedPython binding issue)
   - len, sum, min, max, sorted, zip, map, filter
   - dict, list, set, tuple
   - str, int, float, bool

4. Execution Timeout:
   - 160 seconds per execution
   - Prevents infinite loops
   - Enforced by concurrent.futures.TimeoutError

5. Resource Limits:
   - Memory: Inherited from process (no explicit limit)
   - CPU: Single-threaded (no multiprocessing)

Example Sandbox Execution:
```python
import RestrictedPython

code = """
df = pd.read_csv('uploads/abc-123_data.csv')
result = df.groupby('region')['revenue'].mean()
"""

# Compile with RestrictedPython
byte_code = compile_restricted(code, '<string>', 'exec')

# Safe globals
safe_globals = {
    '__builtins__': safe_builtins,
    'pd': pandas,
    'np': numpy,
    '_getattr_': getattr,
    '_getitem_': operator.getitem,
}

# Execute
exec(byte_code, safe_globals)

# Extract result
result = safe_globals.get('result')
```


LAYER 4: PII REDACTION
------------------------

File: app/backend/agents/security.py

Function: redact_pii(text)

Patterns:
1. Email: \b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b
   Example: john.doe@example.com → [REDACTED_EMAIL]

2. Phone (US): \b\d{3}[-.]?\d{3}[-.]?\d{4}\b
   Example: 555-123-4567 → [REDACTED_PHONE]

3. SSN: \b\d{3}-\d{2}-\d{4}\b
   Example: 123-45-6789 → [REDACTED_SSN]

4. Credit Card: \b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b
   Example: 1234 5678 9012 3456 → [REDACTED_CC]

Applied to:
- Audit logs
- Error messages
- User-provided context

Production TODO:
- Replace with ML-based presidio-analyzer for better accuracy
- Detect more PII types (addresses, names, etc.)


LAYER 5: AUDIT LOGGING
------------------------

File: app/backend/agents/security.py

Function: log_audit_event(event_type, submission_id, metadata)

Log Format: JSONL (JSON Lines)
File: audit_logs/audit_YYYY-MM-DD.jsonl

Log Entry Structure:
{
  "timestamp": "2025-12-14T10:30:00.000Z",
  "event_type": "submission_created|llm_invocation|submission_completed",
  "submission_id": "abc-123-def-456",
  "prompt_hash": "sha256_hash_of_prompt",
  "response_hash": "sha256_hash_of_response",
  "metadata": {
    "request_type": "Price Strategy",
    "question_length": 50,
    "file_count": 1,
    "priority": "normal"
  }
}

Events Logged:
1. submission_created: New submission received
2. llm_invocation: LLM API call made
3. submission_completed: Analysis finished
4. submission_error: Error occurred
5. quota_exceeded: Rate limit hit

PII Redaction:
- All text fields redacted before logging
- Hashes used for sensitive content

Retention:
- Daily rotation (new file per day)
- Recommended: Archive after 90 days


LAYER 6: RATE LIMITING
------------------------

File: app/backend/agents/security.py

Quota Management:

Configuration:
- SUBMISSIONS_QUOTA: 100 per hour (default)
- DEEP_ANALYSIS_QUOTA: 10 per hour (deprecated)

Storage: In-memory dictionary
Production TODO: Use Redis or database for persistence

Structure:
```python
quota_tracker = {
  "user_123": {
    "submissions": [
      ("2025-12-14T10:00:00Z", "submission_id_1"),
      ("2025-12-14T10:15:00Z", "submission_id_2"),
      ...
    ],
    "deep_analysis": [...]
  }
}
```

Check Logic:
1. Get user submissions in last hour
2. Count submissions
3. If count >= quota:
     raise HTTPException(429, "Quota exceeded")
   else:
     Allow submission
     Add to tracker

Cleanup:
- Remove entries older than 1 hour
- Run cleanup every 10 minutes


SECURITY BEST PRACTICES
-------------------------

1. Never trust user input:
   ✓ Validate all inputs
   ✓ Sanitize file paths
   ✓ Escape SQL queries (use parameterized queries)

2. Principle of least privilege:
   ✓ Sandbox executes with minimal permissions
   ✓ No file system access
   ✓ No network access

3. Defense in depth:
   ✓ Multiple validation layers
   ✓ RestrictedPython + timeout
   ✓ Audit logging for forensics

4. Secure defaults:
   ✓ Rate limiting enabled by default
   ✓ PII redaction enabled
   ✓ HTTPS recommended for production

5. Monitoring:
   ✓ Audit logs for security events
   ✓ Track quota usage
   ✓ Alert on suspicious patterns


================================================================================
10. CONFIGURATION AND DEPLOYMENT
================================================================================

ENVIRONMENT VARIABLES
----------------------

File: .env (not in repository)

Required Variables:
- ANTHROPIC_API_KEY: Claude API key
  Example: ANTHROPIC_API_KEY=sk-ant-api03-...

Optional Variables:
- DB_URL: Database connection string
  Default: sqlite:///db.sqlite
  Example: DB_URL=postgresql://user:pass@host:5432/db

- BACKEND_URL: Backend server URL
  Default: http://127.0.0.1:8000
  Example: BACKEND_URL=https://api.example.com

- FRONTEND_ORIGIN: Frontend URL for CORS
  Default: http://localhost:8501
  Example: FRONTEND_ORIGIN=https://app.example.com

- API_KEY: Optional API key for endpoint protection
  Example: API_KEY=your-secret-key

- MOCK_EXECUTION: Enable mock mode for testing
  Default: 0
  Example: MOCK_EXECUTION=1

- UPLOAD_DIR: Upload directory path
  Default: uploads
  Example: UPLOAD_DIR=/data/uploads

- SUBMISSIONS_QUOTA: Hourly submission limit
  Default: 100
  Example: SUBMISSIONS_QUOTA=500


DEPENDENCIES
-------------

File: requirements.txt

Key Dependencies:
- fastapi==0.104.1: Web framework
- uvicorn==0.24.0: ASGI server
- streamlit==1.28.2: Frontend UI
- langchain==0.1.0: LLM framework
- langchain-anthropic==0.1.0: Anthropic integration
- langgraph==0.0.20: Agent orchestration
- pandas==2.1.3: Data manipulation
- scikit-learn==1.3.2: Machine learning
- numpy==1.26.2: Numerical computing
- numba==0.58.1: JIT compilation
- RestrictedPython==6.2: Code sandbox
- sqlalchemy==2.0.23: Database ORM
- ydata-profiling==4.5.1: Data profiling
- pytest==7.4.3: Testing framework
- httpx==0.25.2: HTTP client (for tests)

Install:
```bash
pip install -r requirements.txt
```


INITIALIZATION SCRIPTS
-----------------------

1. Setup Environment (PowerShell)
   File: scripts/setup_env.ps1

   ```powershell
   # Check Python 3.10+
   python --version

   # Create virtual environment
   python -m venv .venv

   # Activate
   .venv\Scripts\Activate.ps1

   # Install dependencies
   pip install -r requirements.txt

   echo "Setup complete!"
   ```

2. Initialize Database (PowerShell)
   File: scripts/init_db.ps1

   ```powershell
   # Activate venv
   .venv\Scripts\Activate.ps1

   # Run init script
   python -c "from app.database.database import init_db; init_db()"

   echo "Database initialized!"
   ```

3. Run Backend (PowerShell)
   File: scripts/run_backend.ps1

   ```powershell
   # Activate venv
   .venv\Scripts\Activate.ps1

   # Start server
   uvicorn app.backend.main:app --host 0.0.0.0 --port 8000 --reload
   ```

4. Run Frontend (PowerShell)
   File: scripts/run_frontend.ps1

   ```powershell
   # Activate venv
   .venv\Scripts\Activate.ps1

   # Start Streamlit
   streamlit run app/frontend/app.py
   ```

5. Run Tests (PowerShell)
   File: scripts/run_tests.ps1

   ```powershell
   # Activate venv
   .venv\Scripts\Activate.ps1

   # Run pytest
   pytest tests/ -v
   ```


DOCKER DEPLOYMENT
------------------

File: Dockerfile

```dockerfile
FROM python:3.10-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create upload directory
RUN mkdir -p uploads audit_logs

# Expose port
EXPOSE 8000

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV UPLOAD_DIR=uploads

# Run server
CMD ["uvicorn", "app.backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

Build and Run:
```bash
# Build image
docker build -t csv-agent .

# Run container
docker run -p 8000:8000 \
  -e ANTHROPIC_API_KEY=sk-ant-... \
  -v $(pwd)/uploads:/app/uploads \
  -v $(pwd)/db.sqlite:/app/db.sqlite \
  csv-agent
```

Docker Compose (Optional):
```yaml
version: '3.8'

services:
  backend:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - DB_URL=postgresql://postgres:password@db:5432/csvagent
    volumes:
      - ./uploads:/app/uploads
      - ./audit_logs:/app/audit_logs
    depends_on:
      - db

  db:
    image: postgres:15
    environment:
      - POSTGRES_DB=csvagent
      - POSTGRES_PASSWORD=password
    volumes:
      - pgdata:/var/lib/postgresql/data

volumes:
  pgdata:
```


PRODUCTION DEPLOYMENT CHECKLIST
---------------------------------

1. Security:
   ☐ Set strong API_KEY
   ☐ Use HTTPS (SSL certificate)
   ☐ Configure CORS properly
   ☐ Enable audit logging
   ☐ Set up firewall rules
   ☐ Use environment variables for secrets

2. Database:
   ☐ Switch to PostgreSQL for production
   ☐ Enable database backups
   ☐ Set up connection pooling
   ☐ Create indexes on commonly queried fields

3. Scaling:
   ☐ Use Redis for quota tracking
   ☐ Add Celery for async task processing
   ☐ Deploy multiple backend instances (load balancer)
   ☐ Use CDN for static assets

4. Monitoring:
   ☐ Set up Prometheus metrics
   ☐ Configure Grafana dashboards
   ☐ Enable error tracking (Sentry)
   ☐ Set up alerts for errors and quota exceeded

5. Performance:
   ☐ Enable prompt caching (Claude API)
   ☐ Use connection pooling
   ☐ Configure worker count (uvicorn --workers)
   ☐ Optimize database queries

6. Backup & Recovery:
   ☐ Database backups (daily)
   ☐ Audit log backups (weekly)
   ☐ File upload backups (daily)
   ☐ Test restore procedure


TESTING STRATEGY
-----------------

File: tests/test_scenarios.py

Test Coverage:

1. Unit Tests:
   - Validation agent: profile_data, validate_submission
   - Planning agent: plan_analysis
   - Execution agent: execute_code_sandbox
   - Response agent: generate_response
   - Security: redact_pii, check_quota

2. Integration Tests:
   - API endpoints: POST /submit, GET /result
   - Orchestrator: full PEV workflow
   - Database: CRUD operations

3. End-to-End Tests:
   - Empty CSV: Validation rejection
   - Corrupt CSV: Error handling
   - Valid prediction request: Full workflow
   - Urgent priority: TLDR generation
   - Quota exceeded: Rate limiting

4. Mock Tests:
   - Mock LLM responses to avoid API costs
   - Mock execution for fast tests
   - Use MOCK_EXECUTION=1 environment variable

Run Tests:
```bash
# All tests
pytest tests/ -v

# Specific test
pytest tests/test_scenarios.py::test_empty_csv -v

# With coverage
pytest tests/ --cov=app --cov-report=html
```


KNOWN ISSUES & LIMITATIONS
---------------------------

Current Issues:

1. Print Output Capture:
   - Issue: RestrictedPython binding prevents capturing print()
   - Workaround: Print statements commented out in executed code
   - TODO: Fix RestrictedPython configuration

2. RAG Integration:
   - Issue: FAISS integration stubbed (rag_query_faiss_stub)
   - TODO: Implement FAISS vector store for document retrieval

3. PII Detection:
   - Issue: Uses regex patterns (limited accuracy)
   - TODO: Replace with presidio-analyzer (ML-based)

4. Quota Tracking:
   - Issue: In-memory storage (lost on restart)
   - TODO: Use Redis or database for persistence

5. Artifact Storage:
   - Issue: Visualizations/artifacts removed from response
   - TODO: Re-implement with proper storage (S3, etc.)

Limitations:

1. File Size: 10 MB limit
2. Row Count: 50,000 rows limit
3. Execution Timeout: 160 seconds
4. Single File Support: Primary (multi-file experimental)
5. No User Authentication: TODO for production
6. No Real-time Updates: Uses polling (TODO: WebSockets)


FUTURE ENHANCEMENTS
--------------------

High Priority:
1. Add Celery for async task processing
2. Implement prompt caching (cost reduction)
3. Add user authentication (OAuth/JWT)
4. Switch to PostgreSQL for production
5. Add WebSocket support for real-time updates

Medium Priority:
6. Implement visualization generation
7. Add support for Excel files (.xlsx)
8. Implement multi-file analysis
9. Add support for time series forecasting
10. Implement A/B test analysis

Low Priority:
11. Add support for custom Python libraries
12. Implement data lineage tracking
13. Add support for SQL query generation
14. Implement automated report generation
15. Add support for collaborative analysis


================================================================================
END OF FLOW DOCUMENTATION
================================================================================

This document provides a comprehensive overview of the CSV Analysis
Multi-Agent System, including architecture, data flow, agent interactions,
security features, and deployment procedures.

For questions or issues, please refer to the codebase or contact the
development team.

Last Updated: 2025-12-14
Version: 1.0.0
================================================================================
